@misc{astrach2025Probing,
  title = {Probing Subphonemes in Morphology Models},
  author = {Astrach, Gal and Pinter, Yuval},
  year = {2025},
  month = jun,
  number = {arXiv:2505.11297},
  eprint = {2505.11297},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.11297},
  urldate = {2025-06-18},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computation and Language}
}

@misc{baevski2022Unsupervised,
  title = {Unsupervised Speech Recognition},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
  year = {2022},
  month = may,
  number = {arXiv:2105.11084},
  eprint = {2105.11084},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2105.11084},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{begus2020Generative,
  title = {Generative Adversarial Phonology: Modeling Unsupervised Phonetic and Phonological Learning With Neural Networks},
  shorttitle = {Generative Adversarial Phonology},
  author = {Begu{\v s}, Ga{\v s}per},
  year = {2020},
  month = jul,
  journal = {Frontiers in Artificial Intelligence},
  volume = {3},
  publisher = {Frontiers},
  issn = {2624-8212},
  doi = {10.3389/frai.2020.00044},
  urldate = {2025-07-01},
  langid = {english},
  keywords = {Deep neural network interpretability,Generative Adversarial Networks (GAN),language acquisition,Phonology acquisition,Speech,voice onset time}
}

@inproceedings{begus2020Modeing,
  title = {Modeing Unsupervised Phonetic and Phonological Learning in Generative Adversarial Phonology},
  booktitle = {Proceedings of the Society for Computation in Linguistics 2020},
  author = {Begus, Gasper},
  editor = {Ettinger, Allyson and Jarosz, Gaja and Pater, Joe},
  year = {2020},
  month = jan,
  pages = {38--48},
  publisher = {Association for Computational Linguistics},
  address = {New York, New York},
  urldate = {2025-06-26}
}

@article{benders2023Computational,
  title = {Computational Modelling of Language Acquisition: An Introduction},
  shorttitle = {Computational Modelling of Language Acquisition},
  author = {Benders, Titia and Blom, Elma},
  year = {2023},
  month = nov,
  journal = {Journal of Child Language},
  volume = {50},
  number = {6},
  pages = {1287--1293},
  issn = {0305-0009, 1469-7602},
  doi = {10.1017/S0305000923000429},
  urldate = {2025-07-02},
  langid = {english},
  annotation = {abstractTranslation: 子どもたちが言語を獲得する方法を研究するための1つのアプローチは、計算モデリングを通じて言語習得をシミュレートします。計算モデルは、言語習得の理論とシミュレーションの結果を実装し、既存の実際のデータまたは新しい経験的研究でテストすることができます。 10年以上前に、Journal of Child Languageがトピックに関する特別号を公開し、Brian MacWhinney（Macwhinney、2010）によって編集および紹介されました。したがって、今では、子どもの言語習得の計算モデリングからの最近の研究と洞察を探求する記事のコレクションをまとめることにより、最近の開発を採用する良い時期です。}
}

@book{brentari2019Sign,
  title = {Sign Language Phonology},
  author = {Brentari, Diane},
  year = {2019},
  series = {Key Topics in Phonology},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/9781316286401},
  urldate = {2025-06-23},
  isbn = {978-1-107-11347-3},
  langid = {american}
}

@misc{chen2023Exploring,
  title = {Exploring How Generative Adversarial Networks Learn Phonological Representations},
  author = {Chen, Jingyi and Elsner, Micha},
  year = {2023},
  month = may,
  number = {arXiv:2305.12501},
  eprint = {2305.12501},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.12501},
  urldate = {2025-06-26},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{cho2025Sylber,
  title = {Sylber: Syllabic Embedding Representation of Speech from Raw Audio},
  shorttitle = {Sylber},
  author = {Cho, Cheol Jun and Lee, Nicholas and Gupta, Akshat and Agarwal, Dhruv and Chen, Ethan and Black, Alan W. and Anumanchipalli, Gopala K.},
  year = {2025},
  month = mar,
  number = {arXiv:2410.07168},
  eprint = {2410.07168},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.07168},
  urldate = {2025-06-26},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@misc{choi2024SelfSupervised,
  title = {Self-Supervised Speech Representations Are More Phonetic than Semantic},
  author = {Choi, Kwanghee and Pasad, Ankita and Nakamura, Tomohiko and Fukayama, Satoru and Livescu, Karen and Watanabe, Shinji},
  year = {2024},
  month = jun,
  number = {arXiv:2406.08619},
  eprint = {2406.08619},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08619},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{cruzblandon2023Introducing,
  title = {Introducing Meta-Analysis in the Evaluation of Computational Models of Infant Language Development},
  author = {Cruz Bland{\'o}n, Mar{\'i}a Andrea and Cristia, Alejandrina and R{\"a}s{\"a}nen, Okko},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {7},
  pages = {e13307},
  issn = {1551-6709},
  doi = {10.1111/cogs.13307},
  urldate = {2025-07-02},
  copyright = {{\copyright} 2023 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).},
  langid = {english},
  keywords = {Child language development,Computational modeling,Language acquisition,Meta-analysis,Model evaluation}
}

@article{du2022Phonetically,
  title = {Phonetically Incomplete Neutralisation Can Be Phonologically Complete: Evidence from Huai'an Mandarin},
  shorttitle = {Phonetically Incomplete Neutralisation Can Be Phonologically Complete},
  author = {Du, Naiyan and Durvasula, Karthik},
  year = {2022},
  month = nov,
  journal = {Phonology},
  volume = {39},
  number = {4},
  pages = {559--595},
  issn = {0952-6757, 1469-8188},
  doi = {10.1017/S0952675723000192},
  urldate = {2025-06-23},
  langid = {english},
  keywords = {categorical representations,Huai'an Mandarin,incomplete neutralisation,phonology-phonetics interface,tone sandhi},
  annotation = {abstractTranslation: 不完全な中和の現象は、カテゴリーの音韻中和の推定上の症例が音声的に非中和的であることが観察される状況を表しています。これは、カテゴリの特徴を採用する音韻理論の問題であると主張されています。ここでは、Huai'an MandarinのTone Sandhiプロセスの2つの異なる給餌順序を使用して、不完全な音声中和がカテゴリーの音韻現象と互換性があることを示します。したがって、不完全な音声中和は、勾配音韻表現を自動的に通知しません。さらに、不完全な音声中和が実際に大きな効果サイズを持っている可能性があることを示します。このような結果は、言語のパフォーマンスが多因子問題であると主張されている音韻の古典的な生成的見解から驚くことではなく、言語知識（すなわち、能力）は、関係する多くの要因の1つにすぎません。さらに、我々の結果は、観察された不完全性または勾配が音韻知識の外側のソースを持っている可能性があることを示唆しています。}
}

@article{DuTian2012YinYunLiLunnoWakuZumideYanYudetawoFenXisurutoiukoto,
  title = {音韻理論の枠組みで言語データを分析するということ: 最適性理論を使った音韻獲得データ分析({$<$}小特集{$>$}子どもの音声)},
  shorttitle = {音韻理論の枠組みで言語データを分析するということ},
  author = {都田, 青子},
  year = {2012},
  journal = {日本音響学会誌},
  volume = {68},
  number = {5},
  pages = {254--259},
  doi = {10.20697/jasj.68.5_254}
}

@article{eichhorn2018Effects,
  title = {Effects of Aging on Vocal Fundamental Frequency and Vowel Formants in Men and Women},
  author = {Eichhorn, Julie Traub and Kent, Raymond D. and Austin, Diane and Vorperian, Houri K.},
  year = {2018},
  month = sep,
  journal = {Journal of Voice},
  volume = {32},
  number = {5},
  pages = {644.e1-644.e9},
  issn = {0892-1997},
  doi = {10.1016/j.jvoice.2017.08.003},
  urldate = {2025-07-08},
  keywords = {Adult acoustics,Aging voice,Formants,Fundamental frequency,Sex differences}
}

@article{gosztolya2024Wav2vec,
  title = {Wav2vec 2.0 Embeddings Are No Swiss Army Knife-A Case Study for Multiple Sclerosis},
  author = {Gosztolya, G{\'a}bor and {Kiss-Vetr{\'a}b}, Mercedes and Svindt, Veronika and B{\'o}na, Judit and Hoffmann, Ildik{\'o}},
  year = {2024},
  publisher = {International Speech Communication Association (ISCA)},
  urldate = {2025-07-01}
}

@misc{guriel2023Morphological,
  title = {Morphological Inflection with Phonological Features},
  author = {Guriel, David and Goldman, Omer and Tsarfaty, Reut},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12581},
  eprint = {2306.12581},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.12581},
  urldate = {2025-06-30},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{hayes2008Maximum,
  title = {A Maximum Entropy Model of Phonotactics and Phonotactic Learning},
  author = {Hayes, Bruce and Wilson, Colin},
  year = {2008},
  month = jul,
  journal = {Linguistic Inquiry},
  volume = {39},
  number = {3},
  pages = {379--440},
  issn = {0024-3892, 1530-9150},
  doi = {10.1162/ling.2008.39.3.379},
  urldate = {2025-07-29},
  langid = {english}
}

@inproceedings{higy2021Discrete,
  title = {Discrete Representations in Neural Models of Spoken Language},
  booktitle = {Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  author = {Higy, Bertrand and Gelderloos, Lieke and Alishahi, Afra and Chrupa{\l}a, Grzegorz},
  editor = {Bastings, Jasmijn and Belinkov, Yonatan and Dupoux, Emmanuel and Giulianelli, Mario and Hupkes, Dieuwke and Pinter, Yuval and Sajjad, Hassan},
  year = {2021},
  month = nov,
  pages = {163--176},
  publisher = {Association for Computational Linguistics},
  address = {Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.blackboxnlp-1.11},
  urldate = {2025-07-01}
}

@misc{hsu2021HuBERT,
  title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
  shorttitle = {HuBERT},
  author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  year = {2021},
  month = jun,
  number = {arXiv:2106.07447},
  eprint = {2106.07447},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.07447},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{jarosz2019Computational,
  title = {Computational Modeling of Phonological Learning},
  author = {Jarosz, Gaja},
  year = {2019},
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {67--90},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-011718-011832},
  urldate = {2025-07-02},
  langid = {english}
}

@article{kazanina2018Phonemes,
  title = {Phonemes: Lexical Access and Beyond},
  shorttitle = {Phonemes},
  author = {Kazanina, Nina and Bowers, Jeffrey S. and Idsardi, William},
  year = {2018},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {2},
  pages = {560--585},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1362-0},
  urldate = {2025-06-26},
  langid = {english}
}

@inproceedings{kolachina2019What,
  title = {What Do Phone Embeddings Learn about Phonology?},
  booktitle = {Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology},
  author = {Kolachina, Sudheer and Magyar, Lilla},
  editor = {Nicolai, Garrett and Cotterell, Ryan},
  year = {2019},
  month = aug,
  pages = {160--169},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4219},
  urldate = {2025-06-18},
  langid = {american}
}

@article{maaten2008Visualizing,
  title = {Visualizing Data Using T-SNE},
  author = {van der Maaten, Laurens and Hinton, Geoffrey},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {86},
  pages = {2579--2605},
  issn = {1533-7928},
  urldate = {2025-07-01}
}

@article{mcmurray2023acquisition,
  title = {The Acquisition of Speech Categories: Beyond Perceptual Narrowing, beyond Unsupervised Learning and beyond Infancy},
  shorttitle = {The Acquisition of Speech Categories},
  author = {McMurray, Bob},
  year = {2023},
  month = apr,
  journal = {Language, Cognition and Neuroscience},
  volume = {38},
  number = {4},
  pages = {419--445},
  publisher = {Routledge},
  issn = {2327-3798},
  doi = {10.1080/23273798.2022.2105367},
  urldate = {2025-07-02},
  pmid = {38425732},
  keywords = {category learning,development,infancy,language development,speech categorization,Speech perception,statistical learning,unsupervised learning}
}

@inproceedings{medin2024SelfSupervised,
  title = {Self-Supervised Models for Phoneme Recognition: Applications in Children's Speech for Reading Learning},
  shorttitle = {Self-Supervised Models for Phoneme Recognition},
  booktitle = {Interspeech 2024},
  author = {Medin, Lucas Block and Pellegrini, Thomas and Gelin, Lucile},
  year = {2024},
  month = sep,
  eprint = {2503.04710},
  primaryclass = {cs},
  pages = {5168--5172},
  doi = {10.21437/Interspeech.2024-1095},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{mohamed2022SelfSupervised,
  title = {Self-Supervised Speech Representation Learning: A Review},
  shorttitle = {Self-Supervised Speech Representation Learning},
  author = {Mohamed, Abdelrahman and Lee, Hung-yi and Borgholt, Lasse and Havtorn, Jakob D. and Edin, Joakim and Igel, Christian and Kirchhoff, Katrin and Li, Shang-Wen and Livescu, Karen and Maal{\o}e, Lars and Sainath, Tara N. and Watanabe, Shinji},
  year = {2022},
  month = oct,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {16},
  number = {6},
  eprint = {2205.10643},
  primaryclass = {cs},
  pages = {1179--1210},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2022.3207050},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{nguyen2016Computational,
  title = {Computational Sociolinguistics: A Survey},
  shorttitle = {Computational Sociolinguistics},
  author = {Nguyen, Dong and Do{\u g}ru{\"o}z, A. Seza and Ros{\'e}, Carolyn P. and {de Jong}, Franciska},
  year = {2016},
  month = sep,
  journal = {Computational Linguistics},
  volume = {42},
  number = {3},
  pages = {537--593},
  issn = {0891-2017},
  doi = {10.1162/COLI_a_00258},
  urldate = {2025-07-01}
}

@misc{panchendrarajan2024Synergizing,
  title = {Synergizing Machine Learning \& Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing},
  shorttitle = {Synergizing Machine Learning \& Symbolic Methods},
  author = {Panchendrarajan, Rrubaa and Zubiaga, Arkaitz},
  year = {2024},
  month = mar,
  number = {arXiv:2401.11972},
  eprint = {2401.11972},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.11972},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computation and Language},
  annotation = {abstractTranslation: 機械学習と象徴的なアプローチの進歩は、自然言語処理（NLP）の長所と短所を強調しています。機械学習アプローチはデータのパターンを特定するのに強力ですが、多くの場合、CommonsenseとNLPタスクに必要な事実の知識を学習することに不足しています。一方、象徴的な方法は、知識が豊富なデータを表現することに優れています。しかし、彼らは動的なデータを適応させ、知識を一般化するのに苦労しています。これらの2つのパラダイムをハイブリッドアプローチを介して橋渡しすることで、強さを維持しながら、両方の弱点を軽減できます。最近の研究は、この組合の美徳を称賛し、幅広いNLPタスクで有望な結果を示しています。この論文では、NLPに使用されるハイブリッドアプローチの概要を示します。具体的には、自然言語の理解、生成、推論を必要とする幅広いNLPタスクに使用される最先端のハイブリッドアプローチを掘り下げます。さらに、NLPのハイブリッドアプローチに利用できる既存のリソースと、将来の研究手段のロードマップを提供する課題と将来の方向性について説明します。}
}

@article{pandian2025Hybrid,
  title = {Hybrid Symbolic-Neural Architectures for Explainable Artificial Intelligence in Decision-Critical Domains},
  author = {Pandian, Soundra M},
  year = {2025},
  langid = {english}
}

@misc{parcollet2024LeBenchmark,
  title = {LeBenchmark 2.0: A Standardized, Replicable and Enhanced Framework for Self-Supervised Representations of French Speech},
  shorttitle = {LeBenchmark 2.0},
  author = {Parcollet, Titouan and Nguyen, Ha and Evain, Solene and Boito, Marcely Zanon and Pupier, Adrien and Mdhaffar, Salima and Le, Hang and Alisamir, Sina and Tomashenko, Natalia and Dinarelli, Marco and Zhang, Shucong and Allauzen, Alexandre and Coavoux, Maximin and Esteve, Yannick and Rouvier, Mickael and Goulian, Jerome and Lecouteux, Benjamin and Portet, Francois and Rossato, Solange and Ringeval, Fabien and Schwab, Didier and Besacier, Laurent},
  year = {2024},
  month = mar,
  number = {arXiv:2309.05472},
  eprint = {2309.05472},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.05472},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{pasad2024What,
  title = {What Do Self-Supervised Speech Models Know About Words?},
  author = {Pasad, Ankita and Chien, Chung-Ming and Settle, Shane and Livescu, Karen},
  year = {2024},
  month = apr,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  pages = {372--391},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00656},
  urldate = {2025-07-01},
  langid = {american},
  annotation = {abstractTranslation: 過去数年にわたって多くの自己監視スピーチモデル（S3MS）が導入されており、さまざまな音声タスクのパフォーマンスとデータ効率が向上しています。ただし、これらの経験的成功だけでは、トレーニング前に学んだことの完全な写真を把握していません。最近の作業は、S3MSが音声およびスピーカー情報などの特定のプロパティをエンコードする方法を分析し始めていますが、単語レベルおよびそれ以降でエンコードされた知識の適切な理解がありません。この作業では、軽量分析方法を使用して、S3Mでエンコードされたセグメントレベルの言語特性（ワードアイデンティティ、境界、発音、構文機能、セマンティック機能）を研究します。 10 s3msからの層ごとの表現の比較研究を提示し、（i）各単語セグメント内のフレームレベルの表現はすべて等しく有益ではないことを発見し、（ii）レイヤー全体の言語情報のアクセシビリティと分布にトレーニング前の客観的およびモデルサイズに大きな影響を与えることがわかります。また、視覚的な接地で訓練されたS3MS（単語のセグメンテーション、セマンティックな文の類似性）といういくつかのタスクで、音声のみのカウンターパートよりも優れていることがわかります。最後に、タスクベースの分析は、以前の作業よりも単純な方法を使用しながら、単語のセグメンテーションと音響単語の識別のパフォーマンスの向上を示しています。}
}

@article{pouw2024Perception,
  title = {Perception of Phonological Assimilation by Neural Speech Recognition Models},
  author = {Pouw, Charlotte and Kloots, Marianne de Heer and Alishahi, Afra and Zuidema, Willem},
  year = {2024},
  month = dec,
  journal = {Computational Linguistics},
  volume = {50},
  number = {3},
  pages = {1557--1585},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/coli_a_00526},
  urldate = {2025-07-01}
}

@article{reubold2010Vocal,
  title = {Vocal Aging Effects on {\emph{F}}0 and the First Formant: A Longitudinal Analysis in Adult Speakers},
  shorttitle = {Vocal Aging Effects on {\emph{F}}0 and the First Formant},
  author = {Reubold, Ulrich and Harrington, Jonathan and Kleber, Felicitas},
  year = {2010},
  month = jul,
  journal = {Speech Communication},
  volume = {52},
  number = {7},
  pages = {638--651},
  issn = {0167-6393},
  doi = {10.1016/j.specom.2010.02.012},
  urldate = {2025-07-08},
  keywords = {Formants,Longitudinal analysis,Perception,Source-tract-interaction,Vocal aging}
}

@article{silfverberg2018Sound,
  title = {Sound Analogies with Phoneme Embeddings},
  author = {Silfverberg, Miikka P. and Mao, Lingshuang and Hulden, Mans},
  year = {2018},
  month = jan,
  journal = {Society for Computation in Linguistics},
  volume = {1},
  number = {1},
  publisher = {University of Massachusetts Amherst Libraries},
  issn = {2834-1007},
  doi = {10.7275/R5NZ85VD},
  urldate = {2025-06-26},
  langid = {None}
}

@book{silverman2012Neutralization,
  title = {Neutralization},
  author = {Silverman, Daniel},
  year = {2012},
  publisher = {Cambridge University Press},
  urldate = {2025-07-01}
}

@article{staples2020Neural,
  title = {Neural Components of Reading Revealed by Distributed and Symbolic Computational Models},
  author = {Staples, Ryan and Graves, William W.},
  year = {2020},
  journal = {Neurobiology of Language (Cambridge, Mass.)},
  volume = {1},
  number = {4},
  pages = {381--401},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00018},
  langid = {american},
  pmcid = {PMC9635488},
  pmid = {36339637},
  keywords = {cognitive neuroscience,computational modelling,fMRI,language,orthography,reading},
  annotation = {abstractTranslation: 脳に読書 - 正書法、音韻表現の認知要素がどのようにインスタンス化されるかを決定することは、心理学と人間の認知神経科学の長年の目標となっています。読み取りの2つの最も顕著な計算モデルは、異なる認知プロセスをインスタンス化し、異なる神経プロセスを意味します。読み取りの人工ニューラルネットワーク（ANN）モデルは、非命中性の分散表現を積極的に示しています。代わりに、デュアルルートカスケード（DRC）モデルは、スペルサウンド対応の象徴的なルールを表す2つの処理ルートを示唆しています。これらのモデルは行動データによって裁定されておらず、神経の妥当性の観点からこれまで直接比較されたことはありません。表現類似性分析を使用して、これらのモデルの予測を声を出して読んでいる参加者の神経データと比較しました。 ANNとDRCモデルの両方のモデル表現は、神経活動に対応していました。ただし、ANNモデルの表現は、皮質のより読み取り関連領域と相関しています。 DRCモデルからの寄与が統計的に制御された場合、部分的な相関により、ANNモデルが神経データの有意なばらつきを説明したことが明らかになりました。 ANNモデルからの寄与を考慮してDRCモデルによって説明された分散を調べる反対の分析は、神経活動への対応を明らかにしませんでした。私たちの結果は、ANNSが分散表現を使用して訓練したことは、認知と神経のコーディングの間のより良い対応を提供することを示唆しています。さらに、このフレームワークは、認知機能の計算モデルを比較するための原則的なアプローチを提供し、神経表現に関する洞察を得ることができます。\\
titleTranslation: 分散および象徴的な計算モデルによって明らかにされた読書のニューラル成分}
}

@phdthesis{tesar1995Computational,
  title = {Computational Optimality Theory},
  author = {Tesar, Bruce Benson},
  year = {1995},
  address = {United States -- Colorado},
  urldate = {2025-06-18},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  langid = {american},
  school = {University of Colorado at Boulder},
  keywords = {Applied sciences,Computer science,grammar,language acquisition,Language literature and linguistics,Linguistics},
  annotation = {abstractTranslation: 最適性理論では、言語入力は、候補構造記述の無限のセットから、同じ普遍的な制約のランク付けされたセットを最もよく満たす記述から選択することにより、文法的な構造記述を割り当てられます。言語間変動は、同じ普遍的な制約の異なるランキングとして説明されています。最適性理論の計算的扱い性に関する2つの質問が主に興味深いものです。 1つ目は、最適な構造記述を計算する能力に関するものです。 2番目は、制約ランキングの学習性に関するものです。ダイナミックプログラミングを使用して、最適な形式の計算に対して解析アルゴリズムが表示されます。これらのアルゴリズムは、構造記述内の局所的な情報に基づいて評価される可能性のある普遍的な制約を採用する最適理論の文法に対して機能します。このアプローチは、説明全体から別の説明に移動することでソリューションを検索するのではなく、最適な下部構造を活用して最適な説明を構築します。学習アルゴリズムのクラス、制約降格アルゴリズムが提示され、仮説の構造記述（言語学習の一般的な問題の重要なサブ問題）に基づいて、制約ランキングの学習の問題を解決します。制約降格は、入力の競合する（最適ではない）構造的記述の形で利用可能な暗黙の否定的な証拠を搾取します。このアルゴリズムのデータの複雑さは、制約の数が2次です。}
}

@misc{tsvilodub2025Integrating,
  title = {Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering},
  author = {Tsvilodub, Polina and Hawkins, Robert D. and Franke, Michael},
  year = {2025},
  month = jun,
  number = {arXiv:2506.01474},
  eprint = {2506.01474},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.01474},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  langid = {american},
  keywords = {Computer Science - Computation and Language},
  annotation = {abstractTranslation: 実用的な言語使用の計算モデルは、伝統的に、実現言語の使用に適用可能性を制限しているため、伝統的に手によって指定された発話と意味のセットに依存してきました。 LLMベースのモジュールを統合して自然言語の主要なコンポーネントを提案および評価し、手動仕様の必要性を排除することにより、確率的認知モデルを強化する神経腫瘍フレームワークを提案します。実用的な質問を回答することの古典的なケーススタディを通じて、ユーティリティやリテラルセマンティクスの評価から代替の発話と目標の生成まで、神経モジュールを認知モデルに組み込むためのさまざまなアプローチを体系的に調べます。ハイブリッドモデルは、人間の答えパターンを予測する際に、従来の確率モデルのパフォーマンスに一致またはそれを超えることができます。ただし、神経協力モデルの成功は、LLMの統合方法に大きく依存します。代替案を提案し、抽象的な目標をユーティリティに変換するのに特に効果的ですが、真実の意味評価で課題に直面しています。この作業は、神経と象徴的なコンポーネントのバランスをとるための重要な設計上の考慮事項を照らしながら、より柔軟でスケーラブルな言語使用のモデルへの道を示しています。}
}

@misc{venkateswaran2025Probing,
  title = {Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception},
  shorttitle = {Probing for Phonology in Self-Supervised Speech Representations},
  author = {Venkateswaran, Nitin and Tang, Kevin and Wayland, Ratree},
  year = {2025},
  month = jun,
  number = {arXiv:2506.17542},
  eprint = {2506.17542},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.17542},
  urldate = {2025-07-01},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{YongYe2025LuErDaoFangYanFuHeMingCiakusentonoXinJiuBiJiao,
  title = {鹿児島方言複合名詞アクセントの新旧比較 : 最適性理論を用いた分析},
  shorttitle = {鹿児島方言複合名詞アクセントの新旧比較},
  author = {永野, 颯},
  year = {2025},
  month = jan,
  journal = {言語情報科学},
  volume = {23},
  pages = {37--53},
  publisher = {{Graduate School of Arts and Sciences, the University of Tokyo}},
  issn = {1347-8931},
  doi = {10.15083/0002013353},
  urldate = {2025-06-26},
  keywords = {,Optimality Theory}
}
