<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="">
<meta name="dcterms.date" content="2025-08-19">

<title>Comprehensive Bibliography of Prior Research</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-59898bd1c6b9d2bb783127feaa000c76.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-e310f37c06b14626bd4db825b63c131f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#comprehensive-bibliography-of-prior-research" id="toc-comprehensive-bibliography-of-prior-research" class="nav-link active" data-scroll-target="#comprehensive-bibliography-of-prior-research"><span class="header-section-number">1</span> Comprehensive Bibliography of Prior Research</a>
  <ul class="collapse">
  <li><a href="#i.-theoretical-foundations-of-computational-phonology" id="toc-i.-theoretical-foundations-of-computational-phonology" class="nav-link" data-scroll-target="#i.-theoretical-foundations-of-computational-phonology"><span class="header-section-number">1.1</span> I. Theoretical Foundations of Computational Phonology</a>
  <ul class="collapse">
  <li><a href="#a.-classical-generative-phonology" id="toc-a.-classical-generative-phonology" class="nav-link" data-scroll-target="#a.-classical-generative-phonology"><span class="header-section-number">1.1.1</span> A. Classical Generative Phonology</a></li>
  <li><a href="#b.-optimality-theory-and-constraint-based-phonology" id="toc-b.-optimality-theory-and-constraint-based-phonology" class="nav-link" data-scroll-target="#b.-optimality-theory-and-constraint-based-phonology"><span class="header-section-number">1.1.2</span> B. Optimality Theory and Constraint-Based Phonology</a></li>
  <li><a href="#c.-laboratory-phonology-and-experimental-approaches" id="toc-c.-laboratory-phonology-and-experimental-approaches" class="nav-link" data-scroll-target="#c.-laboratory-phonology-and-experimental-approaches"><span class="header-section-number">1.1.3</span> C. Laboratory Phonology and Experimental Approaches</a></li>
  </ul></li>
  <li><a href="#ii.-neural-network-approaches-to-speech-and-phonology" id="toc-ii.-neural-network-approaches-to-speech-and-phonology" class="nav-link" data-scroll-target="#ii.-neural-network-approaches-to-speech-and-phonology"><span class="header-section-number">1.2</span> II. Neural Network Approaches to Speech and Phonology</a>
  <ul class="collapse">
  <li><a href="#a.-self-supervised-learning-in-speech" id="toc-a.-self-supervised-learning-in-speech" class="nav-link" data-scroll-target="#a.-self-supervised-learning-in-speech"><span class="header-section-number">1.2.1</span> A. Self-Supervised Learning in Speech</a></li>
  <li><a href="#b.-discrete-neural-representations" id="toc-b.-discrete-neural-representations" class="nav-link" data-scroll-target="#b.-discrete-neural-representations"><span class="header-section-number">1.2.2</span> B. Discrete Neural Representations</a></li>
  <li><a href="#c.-neural-symbolic-integration" id="toc-c.-neural-symbolic-integration" class="nav-link" data-scroll-target="#c.-neural-symbolic-integration"><span class="header-section-number">1.2.3</span> C. Neural-Symbolic Integration</a></li>
  </ul></li>
  <li><a href="#iii.-phonological-learning-and-acquisition" id="toc-iii.-phonological-learning-and-acquisition" class="nav-link" data-scroll-target="#iii.-phonological-learning-and-acquisition"><span class="header-section-number">1.3</span> III. Phonological Learning and Acquisition</a>
  <ul class="collapse">
  <li><a href="#a.-computational-models-of-phonological-learning" id="toc-a.-computational-models-of-phonological-learning" class="nav-link" data-scroll-target="#a.-computational-models-of-phonological-learning"><span class="header-section-number">1.3.1</span> A. Computational Models of Phonological Learning</a></li>
  <li><a href="#b.-developmental-phonology-and-corpus-studies" id="toc-b.-developmental-phonology-and-corpus-studies" class="nav-link" data-scroll-target="#b.-developmental-phonology-and-corpus-studies"><span class="header-section-number">1.3.2</span> B. Developmental Phonology and Corpus Studies</a></li>
  <li><a href="#c.-cross-linguistic-phonological-typology" id="toc-c.-cross-linguistic-phonological-typology" class="nav-link" data-scroll-target="#c.-cross-linguistic-phonological-typology"><span class="header-section-number">1.3.3</span> C. Cross-linguistic Phonological Typology</a></li>
  </ul></li>
  <li><a href="#iv.-evaluation-methodologies-and-metrics" id="toc-iv.-evaluation-methodologies-and-metrics" class="nav-link" data-scroll-target="#iv.-evaluation-methodologies-and-metrics"><span class="header-section-number">1.4</span> IV. Evaluation Methodologies and Metrics</a>
  <ul class="collapse">
  <li><a href="#a.-phonological-task-design-and-evaluation" id="toc-a.-phonological-task-design-and-evaluation" class="nav-link" data-scroll-target="#a.-phonological-task-design-and-evaluation"><span class="header-section-number">1.4.1</span> A. Phonological Task Design and Evaluation</a></li>
  <li><a href="#b.-interpretability-and-explainability-metrics" id="toc-b.-interpretability-and-explainability-metrics" class="nav-link" data-scroll-target="#b.-interpretability-and-explainability-metrics"><span class="header-section-number">1.4.2</span> B. Interpretability and Explainability Metrics</a></li>
  <li><a href="#c.-cross-linguistic-evaluation-frameworks" id="toc-c.-cross-linguistic-evaluation-frameworks" class="nav-link" data-scroll-target="#c.-cross-linguistic-evaluation-frameworks"><span class="header-section-number">1.4.3</span> C. Cross-linguistic Evaluation Frameworks</a></li>
  </ul></li>
  <li><a href="#v.-technical-foundations-and-implementation" id="toc-v.-technical-foundations-and-implementation" class="nav-link" data-scroll-target="#v.-technical-foundations-and-implementation"><span class="header-section-number">1.5</span> V. Technical Foundations and Implementation</a>
  <ul class="collapse">
  <li><a href="#a.-deep-learning-architectures-and-optimization" id="toc-a.-deep-learning-architectures-and-optimization" class="nav-link" data-scroll-target="#a.-deep-learning-architectures-and-optimization"><span class="header-section-number">1.5.1</span> A. Deep Learning Architectures and Optimization</a></li>
  <li><a href="#b.-probabilistic-and-statistical-frameworks" id="toc-b.-probabilistic-and-statistical-frameworks" class="nav-link" data-scroll-target="#b.-probabilistic-and-statistical-frameworks"><span class="header-section-number">1.5.2</span> B. Probabilistic and Statistical Frameworks</a></li>
  </ul></li>
  <li><a href="#vi.-interdisciplinary-connections" id="toc-vi.-interdisciplinary-connections" class="nav-link" data-scroll-target="#vi.-interdisciplinary-connections"><span class="header-section-number">1.6</span> VI. Interdisciplinary Connections</a>
  <ul class="collapse">
  <li><a href="#a.-cognitive-science-and-psycholinguistics" id="toc-a.-cognitive-science-and-psycholinguistics" class="nav-link" data-scroll-target="#a.-cognitive-science-and-psycholinguistics"><span class="header-section-number">1.6.1</span> A. Cognitive Science and Psycholinguistics</a></li>
  <li><a href="#b.-philosophy-of-science-and-representational-theory" id="toc-b.-philosophy-of-science-and-representational-theory" class="nav-link" data-scroll-target="#b.-philosophy-of-science-and-representational-theory"><span class="header-section-number">1.6.2</span> B. Philosophy of Science and Representational Theory</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="list_of_references.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Comprehensive Bibliography of Prior Research</h1>
<p class="subtitle lead">Computational Linguistic Study on Japanese Accent Estimation: A Multi-dimensional Literature Survey</p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">true <a href="mailto:s-oswld-n@g.ecc.u-tokyo.ac.jp" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            The University of Tokyo
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 19, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="comprehensive-bibliography-of-prior-research" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Comprehensive Bibliography of Prior Research</h1>
<section id="i.-theoretical-foundations-of-computational-phonology" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="i.-theoretical-foundations-of-computational-phonology"><span class="header-section-number">1.1</span> I. Theoretical Foundations of Computational Phonology</h2>
<section id="a.-classical-generative-phonology" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="a.-classical-generative-phonology"><span class="header-section-number">1.1.1</span> A. Classical Generative Phonology</h3>
<p><strong><span class="citation" data-cites="chomsky1968sound">(<a href="#ref-chomsky1968sound" role="doc-biblioref"><strong>chomsky1968sound?</strong></a>)</span></strong> - Chomsky, Noam, and Morris Halle. <em>The Sound Pattern of English</em>. Harper &amp; Row, 1968. This seminal work establishes the foundation of generative phonology, introducing distinctive feature theory and rule-based phonological processes. The SPE model provides crucial theoretical groundwork for understanding how symbolic representations can capture phonological generalizations, directly relevant to our investigation of optimal representational units.</p>
<p><strong><span class="citation" data-cites="goldsmith1976autosegmental">(<a href="#ref-goldsmith1976autosegmental" role="doc-biblioref"><strong>goldsmith1976autosegmental?</strong></a>)</span></strong> - Goldsmith, John A. “Autosegmental phonology.” PhD dissertation, MIT, 1976. Goldsmith’s autosegmental theory revolutionizes phonological representation by introducing multi-tiered structures. This work is essential for understanding how different phonological features can be represented independently, providing theoretical basis for our multi-dimensional evaluation of representational units.</p>
<p><strong><span class="citation" data-cites="clements1985geometry">(<a href="#ref-clements1985geometry" role="doc-biblioref"><strong>clements1985geometry?</strong></a>)</span></strong> - Clements, George N. “The geometry of phonological features.” <em>Phonology Yearbook</em> 2 (1985): 225-252. This work establishes the hierarchical organization of distinctive features, crucial for understanding how symbolic phonological knowledge can be structured. The geometric approach informs our hybrid architecture design for integrating symbolic constraints with neural representations.</p>
</section>
<section id="b.-optimality-theory-and-constraint-based-phonology" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="b.-optimality-theory-and-constraint-based-phonology"><span class="header-section-number">1.1.2</span> B. Optimality Theory and Constraint-Based Phonology</h3>
<p><strong><span class="citation" data-cites="prince2004optimality">Prince and Smolensky (<a href="#ref-prince2004optimality" role="doc-biblioref">2004</a>)</span></strong> - Prince, Alan, and Paul Smolensky. <em>Optimality Theory: Constraint Interaction in Generative Grammar</em>. Blackwell, 2004. The foundational text of Optimality Theory, establishing constraint-based approaches to phonology. This framework directly informs our Maximum Entropy Harmonic Grammar integration with neural networks, providing the theoretical foundation for our neuro-symbolic hybrid architectures.</p>
<p><strong><span class="citation" data-cites="smolensky2006harmonic">(<a href="#ref-smolensky2006harmonic" role="doc-biblioref"><strong>smolensky2006harmonic?</strong></a>)</span></strong> - Smolensky, Paul, and Géraldine Legendre. <em>The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar</em>. MIT Press, 2006. This work bridges neural computation and symbolic grammar, providing crucial theoretical foundation for our neuro-symbolic integration approach. The harmonic grammar framework offers mathematical tools for parameterizing constraints using neural optimization.</p>
<p><strong><span class="citation" data-cites="hayes2008maxent">(<a href="#ref-hayes2008maxent" role="doc-biblioref"><strong>hayes2008maxent?</strong></a>)</span></strong> - Hayes, Bruce, and Colin Wilson. “A maximum entropy model of phonotactics and phonotactic learning.” <em>Linguistic Inquiry</em> 39.3 (2008): 379-440. Establishes Maximum Entropy approaches to phonological learning, providing computational methods for constraint weight optimization that directly inform our neural constraint parameterization methodology.</p>
</section>
<section id="c.-laboratory-phonology-and-experimental-approaches" class="level3" data-number="1.1.3">
<h3 data-number="1.1.3" class="anchored" data-anchor-id="c.-laboratory-phonology-and-experimental-approaches"><span class="header-section-number">1.1.3</span> C. Laboratory Phonology and Experimental Approaches</h3>
<p><strong><span class="citation" data-cites="pierrehumbert2001exemplar">(<a href="#ref-pierrehumbert2001exemplar" role="doc-biblioref"><strong>pierrehumbert2001exemplar?</strong></a>)</span></strong> - Pierrehumbert, Janet B. “Exemplar dynamics: Word frequency, lenition and contrast.” <em>Frequency and the emergence of linguistic structure</em> 45 (2001): 137-157. Introduces exemplar-based approaches to phonological representation, relevant to our investigation of continuous versus discrete representations and cognitive plausibility validation through usage-based learning patterns.</p>
<p><strong><span class="citation" data-cites="johnson2007decisions">(<a href="#ref-johnson2007decisions" role="doc-biblioref"><strong>johnson2007decisions?</strong></a>)</span></strong> - Johnson, Keith. “Decisions and mechanisms in exemplar-based phonology.” <em>Oxford Handbook of Laboratory Phonology</em> (2007): 25-40. Provides experimental validation of exemplar approaches, offering methodological insights for our cognitive plausibility studies and human-AI comparison protocols.</p>
</section>
</section>
<section id="ii.-neural-network-approaches-to-speech-and-phonology" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="ii.-neural-network-approaches-to-speech-and-phonology"><span class="header-section-number">1.2</span> II. Neural Network Approaches to Speech and Phonology</h2>
<section id="a.-self-supervised-learning-in-speech" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="a.-self-supervised-learning-in-speech"><span class="header-section-number">1.2.1</span> A. Self-Supervised Learning in Speech</h3>
<p><strong><span class="citation" data-cites="baevski2020wav2vec">Baevski et al. (<a href="#ref-baevski2020wav2vec" role="doc-biblioref">2020</a>)</span></strong> - Baevski, Alexei, et al.&nbsp;“wav2vec 2.0: A framework for self-supervised learning of speech representations.” <em>Advances in Neural Information Processing Systems</em> 33 (2020): 12449-12460. This landmark paper establishes the current state-of-the-art in self-supervised speech representation learning. The wav2vec 2.0 framework provides the foundation for our neural representation baseline and is central to our hybrid architecture development.</p>
<p><strong><span class="citation" data-cites="hsu2021hubert">Hsu et al. (<a href="#ref-hsu2021hubert" role="doc-biblioref">2021</a>)</span></strong> - Hsu, Wei-Ning, et al.&nbsp;“HuBERT: Self-supervised speech representation learning by masked prediction of hidden units.” <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 29 (2021): 3451-3460. Introduces masked language modeling approaches to speech representation learning, offering alternative neural baseline architectures and informing our discrete representation strategies through hidden unit prediction.</p>
<p><strong><span class="citation" data-cites="chen2022wavlm">Chen et al. (<a href="#ref-chen2022wavlm" role="doc-biblioref">2022</a>)</span></strong> - Chen, Sanyuan, et al.&nbsp;“WavLM: Large-scale self-supervised pre-training for full stack speech processing.” <em>IEEE Journal of Selected Topics in Signal Processing</em> 16.6 (2022): 1505-1518. Presents advanced self-supervised learning techniques for comprehensive speech processing, providing methodological insights for our cross-linguistic evaluation and scalability analysis.</p>
</section>
<section id="b.-discrete-neural-representations" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="b.-discrete-neural-representations"><span class="header-section-number">1.2.2</span> B. Discrete Neural Representations</h3>
<p><strong><span class="citation" data-cites="oord2017neural">Oord, Vinyals, and Kavukcuoglu (<a href="#ref-oord2017neural" role="doc-biblioref">2017</a>)</span></strong> - van den Oord, Aaron, Oriol Vinyals, and Koray Kavukcuoglu. “Neural discrete representation learning.” <em>Advances in Neural Information Processing Systems</em> 30 (2017): 6306-6315. Introduces Vector Quantized Variational Autoencoders (VQ-VAE), providing the technical foundation for our discrete neural representation approach and bridge between continuous neural representations and symbolic phonological units.</p>
<p><strong><span class="citation" data-cites="razavi2019generating">(<a href="#ref-razavi2019generating" role="doc-biblioref"><strong>razavi2019generating?</strong></a>)</span></strong> - Razavi, Ali, Aaron van den Oord, and Oriol Vinyals. “Generating diverse high-fidelity images with VQ-VAE-2.” <em>Advances in Neural Information Processing Systems</em> 32 (2019): 14866-14876. Advances vector quantization techniques with hierarchical representations, informing our multi-level discrete representation strategies and providing technical methodologies for our hybrid architecture design.</p>
<p><strong><span class="citation" data-cites="dhariwal2020jukebox">(<a href="#ref-dhariwal2020jukebox" role="doc-biblioref"><strong>dhariwal2020jukebox?</strong></a>)</span></strong> - Dhariwal, Prafulla, et al.&nbsp;“Jukebox: A generative model for music.” <em>arXiv preprint arXiv:2005.00341</em> (2020). Demonstrates large-scale application of VQ-VAE to audio generation, providing practical insights for scalability and implementation of discrete neural representations in our phonological modeling framework.</p>
</section>
<section id="c.-neural-symbolic-integration" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="c.-neural-symbolic-integration"><span class="header-section-number">1.2.3</span> C. Neural-Symbolic Integration</h3>
<p><strong><span class="citation" data-cites="garcez2002neural">(<a href="#ref-garcez2002neural" role="doc-biblioref"><strong>garcez2002neural?</strong></a>)</span></strong> - Garcez, Artur S. d’Avila, Krysia B. Broda, and Dov M. Gabbay. <em>Neural-symbolic learning systems: foundations and applications</em>. Springer, 2002. Provides theoretical foundations for neural-symbolic integration, offering frameworks for combining symbolic knowledge with neural learning that directly inform our hybrid architecture design principles.</p>
<p><strong><span class="citation" data-cites="lamb2020graph">(<a href="#ref-lamb2020graph" role="doc-biblioref"><strong>lamb2020graph?</strong></a>)</span></strong> - Lamb, Luis C., et al.&nbsp;“Graph neural networks meet neural-symbolic computing: a survey and perspective.” <em>arXiv preprint arXiv:2003.00330</em> (2020). Surveys modern approaches to neural-symbolic integration using graph neural networks, providing technical methodologies relevant to our constraint graph representations and symbolic knowledge integration.</p>
</section>
</section>
<section id="iii.-phonological-learning-and-acquisition" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="iii.-phonological-learning-and-acquisition"><span class="header-section-number">1.3</span> III. Phonological Learning and Acquisition</h2>
<section id="a.-computational-models-of-phonological-learning" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="a.-computational-models-of-phonological-learning"><span class="header-section-number">1.3.1</span> A. Computational Models of Phonological Learning</h3>
<p><strong><span class="citation" data-cites="tesar2004learning">(<a href="#ref-tesar2004learning" role="doc-biblioref"><strong>tesar2004learning?</strong></a>)</span></strong> - Tesar, Bruce, and Paul Smolensky. “Learning optimality-theoretic grammars.” <em>Linguistic Inquiry</em> 35.4 (2004): 503-530. Establishes computational approaches to learning OT grammars, providing algorithmic foundations for our constraint learning methodologies and validation frameworks for acquired phonological knowledge.</p>
<p><strong><span class="citation" data-cites="jarosz2006rich">(<a href="#ref-jarosz2006rich" role="doc-biblioref"><strong>jarosz2006rich?</strong></a>)</span></strong> - Jarosz, Gaja. “Rich lexicons and restrictive grammars: Maximum likelihood and Bayesian learning in Optimality Theory.” PhD dissertation, Johns Hopkins University, 2006. Develops probabilistic approaches to OT learning, offering mathematical frameworks for our Maximum Entropy constraint optimization and providing methodologies for handling uncertainty in phonological learning.</p>
<p><strong><span class="citation" data-cites="yang2002knowledge">(<a href="#ref-yang2002knowledge" role="doc-biblioref"><strong>yang2002knowledge?</strong></a>)</span></strong> - Yang, Charles D. “Knowledge and learning in natural language.” <em>PhD dissertation, MIT</em>, 2002. Investigates knowledge acquisition in phonological learning from a computational perspective, informing our cognitive plausibility validation studies and providing theoretical background for our developmental simulation methodologies.</p>
</section>
<section id="b.-developmental-phonology-and-corpus-studies" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="b.-developmental-phonology-and-corpus-studies"><span class="header-section-number">1.3.2</span> B. Developmental Phonology and Corpus Studies</h3>
<p><strong><span class="citation" data-cites="macwhinney2000childes">(<a href="#ref-macwhinney2000childes" role="doc-biblioref"><strong>macwhinney2000childes?</strong></a>)</span></strong> - MacWhinney, Brian. “The CHILDES project: Tools for analyzing talk.” <em>Lawrence Erlbaum Associates</em>, 2000. Establishes the CHILDES corpus framework that serves as our primary data source for cognitive plausibility validation and developmental pattern analysis in phonological acquisition.</p>
<p><strong><span class="citation" data-cites="demuth2006prosodic">(<a href="#ref-demuth2006prosodic" role="doc-biblioref"><strong>demuth2006prosodic?</strong></a>)</span></strong> - Demuth, Katherine. “Prosodic licensing in children’s development.” <em>Language</em> 82.3 (2006): 610-616. Provides developmental insights into phonological acquisition patterns, informing our cognitive plausibility validation metrics and offering empirical benchmarks for our developmental simulation studies.</p>
<p><strong><span class="citation" data-cites="fikkert1994acquisition">(<a href="#ref-fikkert1994acquisition" role="doc-biblioref"><strong>fikkert1994acquisition?</strong></a>)</span></strong> - Fikkert, Paula. “On the acquisition of prosodic structure.” PhD dissertation, Leiden University, 1994. Investigates prosodic development in early language acquisition, providing theoretical framework for our cognitive plausibility studies and methodological approaches for evaluating acquisition-consistent representations.</p>
</section>
<section id="c.-cross-linguistic-phonological-typology" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="c.-cross-linguistic-phonological-typology"><span class="header-section-number">1.3.3</span> C. Cross-linguistic Phonological Typology</h3>
<p><strong><span class="citation" data-cites="maddieson1984patterns">(<a href="#ref-maddieson1984patterns" role="doc-biblioref"><strong>maddieson1984patterns?</strong></a>)</span></strong> - Maddieson, Ian. <em>Patterns of sounds</em>. Cambridge University Press, 1984. Comprehensive cross-linguistic survey of phonological systems, providing empirical foundation for our cross-linguistic evaluation methodology and ensuring typological adequacy of our representational frameworks.</p>
<p><strong><span class="citation" data-cites="hayes2009introductory">(<a href="#ref-hayes2009introductory" role="doc-biblioref"><strong>hayes2009introductory?</strong></a>)</span></strong> - Hayes, Bruce. <em>Introductory phonology</em>. Wiley-Blackwell, 2009. Provides comprehensive overview of phonological theory with cross-linguistic perspective, informing our evaluation framework design and ensuring theoretical grounding for our multi-dimensional assessment protocols.</p>
</section>
</section>
<section id="iv.-evaluation-methodologies-and-metrics" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="iv.-evaluation-methodologies-and-metrics"><span class="header-section-number">1.4</span> IV. Evaluation Methodologies and Metrics</h2>
<section id="a.-phonological-task-design-and-evaluation" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="a.-phonological-task-design-and-evaluation"><span class="header-section-number">1.4.1</span> A. Phonological Task Design and Evaluation</h3>
<p><strong><span class="citation" data-cites="schatz2013evaluating">(<a href="#ref-schatz2013evaluating" role="doc-biblioref"><strong>schatz2013evaluating?</strong></a>)</span></strong> - Schatz, Thomas, et al.&nbsp;“Evaluating speech features with the minimal-pair ABX task: Analysis of the classical MFC/PLP pipeline.” <em>Proceedings of Interspeech</em> (2013): 1781-1785. Establishes ABX discrimination tasks for phonological evaluation, providing methodological foundation for our cognitive plausibility validation and offering standardized protocols for human-AI comparison studies.</p>
<p><strong><span class="citation" data-cites="dunbar2017zero">(<a href="#ref-dunbar2017zero" role="doc-biblioref"><strong>dunbar2017zero?</strong></a>)</span></strong> - Dunbar, Ewan, et al.&nbsp;“The zero resource speech challenge 2017.” <em>IEEE Workshop on Automatic Speech Recognition and Understanding</em> (2017): 323-330. Provides comprehensive evaluation frameworks for unsupervised speech processing, informing our baseline evaluation methodologies and offering comparative benchmarks for representational unit assessment.</p>
<p><strong><span class="citation" data-cites="versteegh2015zero">(<a href="#ref-versteegh2015zero" role="doc-biblioref"><strong>versteegh2015zero?</strong></a>)</span></strong> - Versteegh, Maarten, et al.&nbsp;“The zero resource speech challenge 2015: Proposed approaches and results.” <em>Procedia Computer Science</em> 81 (2016): 67-72. Establishes zero-resource evaluation protocols relevant to our self-supervised learning approaches and providing methodological frameworks for cross-linguistic evaluation without supervised resources.</p>
</section>
<section id="b.-interpretability-and-explainability-metrics" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="b.-interpretability-and-explainability-metrics"><span class="header-section-number">1.4.2</span> B. Interpretability and Explainability Metrics</h3>
<p><strong><span class="citation" data-cites="linzen2016assessing">(<a href="#ref-linzen2016assessing" role="doc-biblioref"><strong>linzen2016assessing?</strong></a>)</span></strong> - Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. “Assessing the ability of LSTMs to learn syntax-sensitive dependencies.” <em>Transactions of the Association for Computational Linguistics</em> 4 (2016): 521-535. Develops probing methodologies for assessing linguistic knowledge in neural networks, providing technical approaches for our interpretability analysis and linguistic knowledge preservation assessment.</p>
<p><strong><span class="citation" data-cites="belinkov2019analysis">(<a href="#ref-belinkov2019analysis" role="doc-biblioref"><strong>belinkov2019analysis?</strong></a>)</span></strong> - Belinkov, Yonatan, and James Glass. “Analysis methods in neural language processing: A survey.” <em>Transactions of the Association for Computational Linguistics</em> 7 (2019): 49-72. Comprehensive survey of neural network analysis techniques, informing our interpretability evaluation methodology and providing technical frameworks for assessing symbolic knowledge preservation in hybrid architectures.</p>
</section>
<section id="c.-cross-linguistic-evaluation-frameworks" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="c.-cross-linguistic-evaluation-frameworks"><span class="header-section-number">1.4.3</span> C. Cross-linguistic Evaluation Frameworks</h3>
<p><strong><span class="citation" data-cites="comrie1989language">(<a href="#ref-comrie1989language" role="doc-biblioref"><strong>comrie1989language?</strong></a>)</span></strong> - Comrie, Bernard. <em>Language universals and linguistic typology: Syntax and morphology</em>. University of Chicago Press, 1989. Provides theoretical framework for cross-linguistic evaluation, ensuring typological adequacy of our evaluation protocols and informing our language sampling methodology for comprehensive validation.</p>
<p><strong><span class="citation" data-cites="dryer2013world">(<a href="#ref-dryer2013world" role="doc-biblioref"><strong>dryer2013world?</strong></a>)</span></strong> - Dryer, Matthew S., and Martin Haspelmath, eds.&nbsp;<em>The world atlas of language structures online</em>. Max Planck Institute for Evolutionary Anthropology, 2013. Comprehensive cross-linguistic database informing our language selection for evaluation and providing empirical foundation for ensuring typological diversity in our validation studies.</p>
</section>
</section>
<section id="v.-technical-foundations-and-implementation" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="v.-technical-foundations-and-implementation"><span class="header-section-number">1.5</span> V. Technical Foundations and Implementation</h2>
<section id="a.-deep-learning-architectures-and-optimization" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="a.-deep-learning-architectures-and-optimization"><span class="header-section-number">1.5.1</span> A. Deep Learning Architectures and Optimization</h3>
<p><strong><span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref"><strong>vaswani2017attention?</strong></a>)</span></strong> - Vaswani, Ashish, et al.&nbsp;“Attention is all you need.” <em>Advances in Neural Information Processing Systems</em> 30 (2017): 5998-6008. Introduces transformer architectures that underlie modern self-supervised speech models, providing technical foundation for our neural baseline implementations and hybrid architecture design.</p>
<p><strong><span class="citation" data-cites="kingma2013auto">(<a href="#ref-kingma2013auto" role="doc-biblioref"><strong>kingma2013auto?</strong></a>)</span></strong> - Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” <em>arXiv preprint arXiv:1312.6114</em> (2013). Establishes variational autoencoder frameworks that inform our discrete representation learning approach and provide mathematical foundation for our VQ-VAE implementation.</p>
</section>
<section id="b.-probabilistic-and-statistical-frameworks" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="b.-probabilistic-and-statistical-frameworks"><span class="header-section-number">1.5.2</span> B. Probabilistic and Statistical Frameworks</h3>
<p><strong><span class="citation" data-cites="bishop2006pattern">(<a href="#ref-bishop2006pattern" role="doc-biblioref"><strong>bishop2006pattern?</strong></a>)</span></strong> - Bishop, Christopher M. <em>Pattern recognition and machine learning</em>. Springer, 2006. Provides comprehensive mathematical foundation for machine learning approaches used throughout our research, including probabilistic modeling, optimization theory, and statistical evaluation methodologies.</p>
<p><strong><span class="citation" data-cites="murphy2012machine">(<a href="#ref-murphy2012machine" role="doc-biblioref"><strong>murphy2012machine?</strong></a>)</span></strong> - Murphy, Kevin P. <em>Machine learning: a probabilistic perspective</em>. MIT Press, 2012. Offers detailed treatment of probabilistic machine learning relevant to our Maximum Entropy approaches and statistical validation methodologies.</p>
</section>
</section>
<section id="vi.-interdisciplinary-connections" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="vi.-interdisciplinary-connections"><span class="header-section-number">1.6</span> VI. Interdisciplinary Connections</h2>
<section id="a.-cognitive-science-and-psycholinguistics" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="a.-cognitive-science-and-psycholinguistics"><span class="header-section-number">1.6.1</span> A. Cognitive Science and Psycholinguistics</h3>
<p><strong><span class="citation" data-cites="mcclelland1986parallel">(<a href="#ref-mcclelland1986parallel" role="doc-biblioref"><strong>mcclelland1986parallel?</strong></a>)</span></strong> - McClelland, James L., David E. Rumelhart, and PDP Research Group. “Parallel distributed processing: Explorations in the microstructure of cognition.” MIT Press, 1986. Establishes connectionist approaches to cognitive modeling relevant to our neural-symbolic integration and providing theoretical foundation for our cognitive plausibility validation studies.</p>
<p><strong><span class="citation" data-cites="elman1996rethinking">(<a href="#ref-elman1996rethinking" role="doc-biblioref"><strong>elman1996rethinking?</strong></a>)</span></strong> - Elman, Jeffrey L., et al.&nbsp;<em>Rethinking innateness: A connectionist perspective on development</em>. MIT Press, 1996. Provides theoretical framework for understanding learning and development in neural systems, informing our cognitive plausibility studies and developmental simulation methodologies.</p>
</section>
<section id="b.-philosophy-of-science-and-representational-theory" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="b.-philosophy-of-science-and-representational-theory"><span class="header-section-number">1.6.2</span> B. Philosophy of Science and Representational Theory</h3>
<p><strong><span class="citation" data-cites="fodor1975language">(<a href="#ref-fodor1975language" role="doc-biblioref"><strong>fodor1975language?</strong></a>)</span></strong> - Fodor, Jerry A. <em>The language of thought</em>. Harvard University Press, 1975. Establishes theoretical foundations for symbolic representation in cognitive systems, providing philosophical grounding for our investigation of optimal representational units and symbolic-neural integration.</p>
<p><strong><span class="citation" data-cites="clark1993associative">(<a href="#ref-clark1993associative" role="doc-biblioref"><strong>clark1993associative?</strong></a>)</span></strong> - Clark, Andy. <em>Associative engines: Connectionism, concepts, and representational change</em>. MIT Press, 1993. Explores the relationship between symbolic and connectionist approaches to representation, offering theoretical framework for understanding the integration challenges addressed in our hybrid architectures.</p>
<p>This comprehensive bibliography provides the theoretical, methodological, and empirical foundation necessary for conducting rigorous research into optimal representational units for computational phonology. Each reference contributes specific insights and methodologies that directly inform our multi-dimensional approach to evaluating and integrating symbolic and neural representations in phonological modeling.</p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-baevski2020wav2vec" class="csl-entry" role="listitem">
Baevski, Alexei, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. <span>“Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.”</span> In <em>Advances in Neural Information Processing Systems</em>, 33:12449–60. <a href="https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html</a>.
</div>
<div id="ref-chen2022wavlm" class="csl-entry" role="listitem">
Chen, Sanyuan, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, et al. 2022. <span>“WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing.”</span> <em>IEEE Journal of Selected Topics in Signal Processing</em> 16 (6): 1505–18. <a href="https://doi.org/10.1109/JSTSP.2022.3188113">https://doi.org/10.1109/JSTSP.2022.3188113</a>.
</div>
<div id="ref-hsu2021hubert" class="csl-entry" role="listitem">
Hsu, Wei-Ning, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. <span>“HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.”</span> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> 29: 3451–60. <a href="https://doi.org/10.1109/TASLP.2021.3122291">https://doi.org/10.1109/TASLP.2021.3122291</a>.
</div>
<div id="ref-oord2017neural" class="csl-entry" role="listitem">
Oord, Aaron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2017. <span>“Neural Discrete Representation Learning.”</span> In <em>Advances in Neural Information Processing Systems</em>, 30:6306–15.
</div>
<div id="ref-prince2004optimality" class="csl-entry" role="listitem">
Prince, Alan, and Paul Smolensky. 2004. <span>“Optimality Theory: Constraint Interaction in Generative Grammar.”</span> <em>Optimality Theory in Phonology: A Reader</em>, 1–71.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>