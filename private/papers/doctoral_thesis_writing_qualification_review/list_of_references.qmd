---
title: "Annotated Bibliography of Prior Research"
subtitle: "{{< meta thesis.title.en >}}"
---

# Core Theoretical Foundations

@chomsky1968  
Foundational work establishing the Sound Pattern of English (SPE) framework with binary distinctive features as atomic phonological units. Introduces systematic rule-based derivations transforming underlying representations to surface forms, demonstrating how complex cross-linguistic patterns emerge from finite universal features and context-sensitive rewrite rules. Essential for understanding symbolic phonology traditions.

@prince2004optimality  
Revolutionary constraint-based phonological theory replacing serial derivations with parallel evaluation of competing candidates through ranked, violable constraints. Shows how cross-linguistic variation emerges from different rankings of universal constraints rather than different rules. Key innovation is factorial typology predicting possible and impossible languages. Provides natural bridge to neural implementation through weighted constraints.

@goldsmith1976  
Doctoral dissertation fundamentally reconceptualizing phonological representation through autosegmental theory. Proposes that tones and segments exist on separate autonomous tiers connected by association lines, elegantly explaining previously complex tonal phenomena. Demonstrates how multi-tiered architecture naturally captures spreading, stability, and floating elements. Directly parallels modern neural architectures with separate information streams.

@clements1985  
Introduces hierarchical organization of distinctive features in tree structures capturing dependencies between features. Groups features under organizing nodes (Place, Laryngeal, Manner) defining natural classes and constraining possible processes. Explains why certain features pattern together in assimilation and dissimilation. Raises critical questions about whether neural embeddings naturally develop similar hierarchical structure.

# Self-Supervised Learning in Speech

@baevski2020  
Landmark paper introducing wav2vec 2.0, revolutionizing speech representation learning through self-supervised pretraining. Achieves competitive ASR performance with only 10 minutes of labeled data using contrastive learning objective. Key innovations include quantization module for discretization and masked prediction task. Demonstrates that rich phonological representations emerge from raw speech without explicit supervision.

@hsu2021  
HuBERT (Hidden Unit BERT) introduces iterative refinement approach to self-supervised speech learning without predefined discrete units. Alternates between clustering hidden representations to create pseudo-labels and training new model to predict them. Progressively refines discrete units from acoustic clusters to linguistic categories. Shows superior performance on phonetic discrimination tasks and unit discovery.

@chen2022  
WavLM extends masked speech prediction to handle both recognition and generation tasks through unified pretraining. Key innovations include joint training on speech and noise for robustness, gated relative position bias, and 94k hours diverse training data. Achieves state-of-the-art on SUPERB benchmark across all tasks. Multi-task capabilities suggest representations capture multiple linguistic levels simultaneously.

@mohamed2022  
Comprehensive survey systematically categorizing self-supervised speech methods into generative, contrastive, and predictive paradigms. Analyzes architectural choices, training objectives, and evaluation protocols. Compares performance across phonetic discrimination, word segmentation, and downstream tasks. Identifies open challenges including efficiency, multilingual learning, and interpretability. Provides methodological foundation for systematic representation comparison.

# Vector Quantization and Discrete Representations

@vandenoord2017  
Seminal work introducing Vector Quantized Variational Autoencoder (VQ-VAE) enabling neural networks to learn discrete latent representations. Addresses backpropagation through discrete variables using straight-through estimator and commitment loss. Demonstrates that VQ-VAE discovers meaningful discrete codes without supervision across modalities. Provides principled approach to learning discrete units potentially corresponding to phonological categories.

@zhang2024  
SpeechTokenizer presents hierarchical residual vector quantization explicitly disentangling semantic and acoustic information across layers. First RVQ layer captures content with HuBERT guidance while subsequent layers encode paralinguistic details. Enables flexible generation control using different layer combinations. Demonstrates explicit separation of linguistic and acoustic information crucial for phonological modeling.

@chang2024  
Challenge paper presenting first large-scale systematic comparison of 40+ discrete speech unit submissions across ASR, TTS, and synthesis tasks. Finds SSL-based units outperform codec-based units for linguistic tasks while codecs preserve acoustic details. Establishes standardized evaluation protocols and baselines. Critical for choosing optimal discretization strategy based on task requirements.

@higy2021  
Systematic analysis of discrete representations in neural speech models examining VQ bottlenecks and clustering approaches. Investigates how discretization affects phonological information preservation and model interpretability. Shows that carefully designed quantization maintains performance while enabling symbolic manipulation. Provides practical guidelines for codebook size selection and training strategies.

# Computational Phonology

@hayes2008  
Influential paper presenting Maximum Entropy framework for learning phonotactic grammars from positive data alone. Introduces automatic constraint induction algorithm discovering relevant generalizations without manual specification. Learns weighted constraints assigning probability distributions over sound sequences. Successfully captures gradient well-formedness intuitions. Provides principled probabilistic interpretation enabling neural parameterization while maintaining interpretability.

@tesar1998  
Foundational work establishing computational learning theory for Optimality Theory grammars proving constraint rankings learnable from positive data. Introduces Recursive Constraint Demotion algorithm iteratively demoting violated constraints below satisfied ones. Proves polynomial-time convergence with representative data. Addresses Credit Problem through Minimal Violation principle. Provides crucial baselines for comparing symbolic and neural learning.

@daland2015  
Comprehensive corpus-based evidence demonstrating long-distance phonological dependencies are gradient rather than categorical. Statistical analysis of harmony and cooccurrence restrictions shows continuous probability distributions. Information-theoretic measures quantify dependency strength correlating with well-formedness judgments. Argues gradient patterns emerge from multiple weak constraints. Bridges symbolic phonology with statistical learning.

@jarosz2019  
Comprehensive review of computational phonological learning covering both symbolic and statistical approaches. Discusses hidden structure problems, ambiguous data challenges, and bias-variance tradeoffs. Compares different learning algorithms and representations. Highlights importance of developmental trajectories and cognitive constraints. Provides unified framework for understanding diverse computational approaches to phonological acquisition.

# Neuro-Symbolic Integration

@begus2020gap  
Pioneering work introducing Generative Adversarial Networks to phonological learning without explicit symbolic supervision. Generator learns context-appropriate allophone production through adversarial training. Discovers phonetically natural implementations of phonological processes analyzable through acoustic measurements. Argues phonological knowledge emerges from production-perception tension. Demonstrates neural architectures can discover phonological generalizations autonomously.

@chen2023  
Systematic analysis of phonological knowledge acquired by GANs through carefully designed probing experiments. Finds generators develop discrete intermediate representations corresponding to phonological features and hierarchical processing with surface-underlying distinction. Identifies limitations with opaque interactions and long-distance dependencies. Reveals which phenomena neural networks naturally capture versus where symbolic constraints provide necessary bias.

@garcez2022  
Comprehensive book providing theoretical and practical foundations for integrating symbolic reasoning with neural learning. Presents multiple paradigms including knowledge compilation, extraction, and hybrid dynamic interaction. Covers differentiable logic, graph neural networks, and neurosymbolic program synthesis. Case studies demonstrate applications across domains. Methods for maintaining differentiability while enforcing symbolic structure directly inform phonological modeling.

@panchendrarajan2024  
Recent survey examining success patterns of neuro-symbolic integration across NLP tasks. Analyzes 200+ papers categorizing approaches by integration strategy. Finds integrated approaches with differentiably embedded logical constraints most promising. Identifies common failure modes and scalability issues. Guides integration strategy suggesting compiling constraints into neural architectures while maintaining differentiability.

# Phonological Representation Analysis

@silfverberg2018  
Demonstrates distributed phoneme embeddings learned from text encode systematic phonological relationships enabling analogical reasoning. Vector arithmetic captures phonological alternations with voicing and place relationships emerging without explicit supervision. Embeddings successfully predict held-out alternations and transfer cross-linguistically. Shows phonological structure emerges more clearly from phoneme than orthographic sequences.

@kolachina2019  
Systematic probing study investigating phonological knowledge encoded in phone embeddings across tasks and architectures. Tests whether embeddings capture distinctive features, natural classes, and processes using diagnostic classifiers. Finds contextual embeddings capture allophonic variation while static embeddings capture phonemic categories. Geometry correlates with feature-based distances. Reveals limitations with privative features.

@venkateswaran2025  
Comprehensive analysis of phonological knowledge in state-of-the-art SSL models testing representations across layers. Reveals clear hierarchical organization with acoustic details in lower layers and abstract categories in upper layers. Phonological structure emerges most clearly in intermediate layers. Cross-linguistic experiments show transfer to unseen languages. Directly informs layer selection strategies and benchmarks.

@astrach2025  
Investigates subphonemic representations in morphology models examining feature-level encoding below segment level. Uses probing to test for articulatory and acoustic features in neural representations. Finds models capture fine-grained phonetic detail relevant for morphophonological processes. Demonstrates importance of sub-segmental information for modeling alternations and provides visualization techniques.

# Language Acquisition and Cognitive Modeling

@macwhinney2000  
Presents comprehensive Child Language Data Exchange System containing 50+ million words of transcribed conversations in 30+ languages. Details CHAT transcription format encoding phonetic details, gestures, and context. Describes CLAN analysis tools for automated phonological development analysis. Captures gradual development from babbling through complex phonology. Provides ecologically valid training data for computational acquisition models.

@dupoux2018  
Articulates research program using AI systems as models of human cognitive development focusing on language acquisition. Proposes evaluation based on learning trajectories, error patterns, and critical periods. Outlines key phenomena including categorical perception emergence and perceptual narrowing. Introduces cognitive benchmark approach assessing human-like behavior across developmental stages. Essential framework for evaluating cognitive plausibility.

@schatz2021  
Presents comprehensive evaluation metrics and baselines for unsupervised speech learning without transcriptions. Includes ABX discrimination measuring phonemic contrast distinction, word segmentation boundary detection, and syntactic/semantic probing. Provides dynamic time warping implementation and statistical methods for acoustic confound control. Reveals phonetic discrimination emerges early while word representations require more data.

@cruzblandon2023  
Introduces meta-analysis framework for evaluating computational models of infant language development. Synthesizes findings across multiple studies identifying consistent patterns and contradictions. Proposes standardized evaluation protocols for model comparison. Emphasizes importance of ecological validity and developmental trajectories. Provides statistical methods for aggregating results across heterogeneous studies and identifying robust findings.

@benders2023  
Introduction to computational modeling of language acquisition bridging theoretical frameworks with implementation details. Covers different modeling paradigms from symbolic to neural approaches. Discusses challenges in modeling realistic input and developmental constraints. Reviews evaluation methods comparing models with child data. Emphasizes importance of cognitive plausibility beyond task performance.

@mcmurray2023  
Critical analysis challenging traditional categorical perception views in speech. Presents evidence for gradient representations and continuous processing dynamics. Discusses implications for computational models of speech perception and acquisition. Argues against strict categorical boundaries in favor of probabilistic representations. Provides behavioral benchmarks for evaluating model predictions about perceptual categorization.

# Evaluation Methodologies

@conneau2020  
Introduces XLSR scaling wav2vec 2.0 to 53 languages covering diverse phonological systems. Demonstrates multilingual pretraining improves even high-resource language performance. Reveals learned representations capture universal phonetic features while maintaining language-specific information. Shows successful zero-shot transfer to unseen languages. Provides framework and baselines for cross-linguistic evaluation.

@mcauliffe2017  
Montreal Forced Aligner: trainable text-speech alignment system using Kaldi toolkit with speaker adaptation. Provides pretrained models for 50+ languages and tools for custom training. Uses multi-stage process from monophones through triphones with speaker adaptation. Achieves accuracy within 20ms of manual annotations. Essential infrastructure for creating phonetically annotated corpora.

@belinkov2019  
Comprehensive survey systematizing analysis methods for neural NLP models providing methodological toolkit. Categorizes approaches into visualization, diagnostic probing, adversarial evaluation, and behavioral analysis. Discusses probe complexity, significance testing, and correlation versus causation problems. Provides implementation guidelines and limitation discussions. Establishes best practices for analyzing phonological representations.

@panayotov2015  
LibriSpeech: Large-scale ASR corpus with 1000 hours of read English audiobooks with aligned transcriptions. Carefully designed train/dev/test splits avoiding speaker overlap. Provides clean and other conditions with varying acoustic quality. Includes language models and baseline recipes. Standard benchmark enabling fair comparison across representation learning approaches.

# Recent Advances and Applications

@yang2024  
k2SSL introduces highly optimized SSL framework achieving 34.8% WER reduction with 3.5x faster training. Key innovation is Zipformer architecture using U-Net style downsampling reducing sequence length. Additional optimizations include ScaledAdam, progressive training, and improved augmentation. Demonstrates efficiency and performance aren't mutually exclusive. Shows architectural innovations improve phonological learning while reducing computation.

@liu2022  
Demonstrates SSL representations predict human hearing thresholds and speech recognition without task-specific training. Middle layers best predict audiometric thresholds while upper layers correlate with recognition. Models implicitly learn frequency selectivity similar to human auditory filters. Captures individual perception differences. Demonstrates practical benefits and provides perceptual grounding for representation evaluation.

@ebrahimi2023  
Critical review evaluating neuro-symbolic integration success across 200+ NLP papers. Categorizes approaches finding integrated methods with differentiable logical constraints most promising. Identifies common failure modes including optimization difficulties and scalability issues. Provides insights guiding integration strategy. Suggests compiling constraints into neural architectures while maintaining differentiability.

@cho2025  
Sylber introduces syllabic embedding representation learning from raw audio improving efficiency. Uses syllable-level discretization reducing sequence length while preserving linguistic information. Demonstrates advantages for downstream tasks requiring prosodic information. Shows intermediate granularity between phones and words optimal for certain applications. Provides evidence for multi-granular representation benefits.

# Cross-linguistic and Typological Studies

@mortensen2016  
Panphon: comprehensive database mapping 5000+ IPA segments to vectors of 21 articulatory features. Combines binary features with gradient phonetic properties enabling categorical and continuous representations. Includes algorithms for feature inference and segment distance calculation. Validates high agreement with linguist judgments. Provides ground-truth for evaluating whether neural models discover articulatory structure.

@phoible  
Comprehensive database aggregating 2155 phonological inventories from 1672 languages worldwide. Includes segment lists with IPA transcriptions, distinctive features, and prosodic information. Covers all language families enabling typological generalizations about universals and variation. Reveals statistical tendencies and implicational relationships. Enables testing whether learned representations capture typological universals.

@dunbar2019  
Zero Resource Challenge evaluating unsupervised linguistic unit discovery from raw speech without text. Uses TTS as downstream task assessing representation quality. Includes typologically diverse languages testing universality. Combines objective metrics with subjective naturalness assessment. Shows different systems discover different granularities. Provides evidence about units emerging from unsupervised learning.

@parcollet2024  
LeBenchmark: comprehensive evaluation framework for French speech processing including detailed phonetic tasks. Provides standardized benchmarks, pretrained models, and evaluation protocols. Covers diverse tasks from phoneme recognition to semantic understanding. Enables systematic comparison across architectures and training approaches. Demonstrates importance of language-specific evaluation beyond English.

# Additional Foundational References

@tesar1995  
Doctoral dissertation providing computational implementation of Optimality Theory demonstrating learnability of constraint rankings. Introduces Robust Interpretive Parsing handling hidden structure in learning. Develops Error-Driven Constraint Demotion algorithm with convergence proofs. Shows how symbolic grammars can be learned from data. Foundational for understanding computational OT.

@silverman2012  
Comprehensive theoretical treatment of neutralization phenomena examining phonological and phonetic aspects. Discusses complete versus incomplete neutralization and implications for phonological theory. Provides cross-linguistic typology of neutralization patterns. Addresses controversies about underlying representations and abstractness. Important for understanding categorical versus gradient phenomena.

@staples2020  
Provides acoustic and articulatory evidence for gradient allophonic variation challenging strict categorical views. Uses ultrasound and acoustic analysis showing continuous variation in supposedly categorical processes. Demonstrates speaker-specific and context-dependent gradience. Argues for probabilistic representations capturing variation. Supports need for representations spanning categorical to gradient.

@nguyen2016  
Analyzes how phonological variation, optionality, and probability are learned in acquisition and diachrony. Presents computational models of variation learning from ambiguous input. Shows how probabilistic patterns emerge from competing constraints. Discusses implications for phonological theory and acquisition. Bridges categorical grammar with probabilistic implementation.

@reubold2010  
Longitudinal study of vocal aging effects on fundamental frequency and formants with normalization implications. Documents systematic changes in acoustic parameters across lifespan. Discusses challenges for speaker normalization in ASR systems. Provides data on within-speaker variation over time. Important for understanding speaker variability in representation learning.

@kazanina2018  
Critical review examining phoneme concept from psychological, neuroscientific, and computational perspectives. Discusses evidence for phonemes in lexical access and speech perception. Reviews controversies about psychological reality of phonological units. Synthesizes findings from multiple methodologies. Provides balanced view of phoneme status in cognitive system.

@tsvilodub2025  
Recent advances in neural-symbolic integration for linguistic structure learning combining pattern recognition with reasoning. Develops modular architectures separating perception from symbolic computation. Shows benefits of structured representations for compositional generalization. Demonstrates successful integration in question-answering systems. Provides architectural blueprints for phonological applications.

@pandian2025  
Examines hybrid symbolic-neural architectures for explainable AI in decision-critical domains. Develops methods for extracting interpretable rules from neural networks. Shows how symbolic knowledge guides neural learning. Addresses trust and verification in AI systems. Directly relevant for interpretable phonological models.

@medin2024  
Explores educational applications of phonetic analysis for language learning and pronunciation training. Develops systems providing explicit feedback on pronunciation errors. Uses SSL models for detailed phonetic assessment. Shows benefits of interpretable representations for pedagogical applications. Demonstrates practical value of phonologically-informed models.

@pouw2024  
Analyzes allophonic variation patterns in spontaneous speech using large corpora and neural models. Tests whether models capture context-dependent variation similar to human productions. Examines gradience in supposedly categorical processes. Provides benchmarks for evaluating allophonic knowledge. Shows importance of naturalistic data.

@guriel2023  
Investigates morphological inflection models incorporating phonological features for improved generalization. Shows explicit phonological representations improve performance on novel forms. Develops architectures combining morphological and phonological processing. Demonstrates benefits of linguistic structure. Provides evidence for integrated morphophonological representations.

@gosztolya2024  
Analysis revealing SSL embeddings limitations for specialized speech tasks like pathological speech assessment. Shows domain-specific features sometimes outperform general SSL representations. Identifies conditions where specialized representations necessary. Provides cautionary evidence about universal applicability. Important for understanding representation limitations.

@pasad2024  
Enhanced LibriSpeech annotations adding detailed phonetic alignments and linguistic features for analysis. Provides frame-level phonetic labels and prosodic annotations. Enables fine-grained evaluation of phonetic representations. Includes speaker metadata for normalization studies. Standard resource for detailed phonetic evaluation.

# References
