---
title: "Writing Plan for Doctoral Thesis"
subtitle: "{{< meta thesis.title.en >}}"
---

# Comprehensive Writing Schedule and Research Implementation Plan

The doctoral thesis investigating optimal representational units for language modeling in computational phonology will be completed following a systematic three-phase approach designed to ensure rigorous empirical investigation, theoretical contribution, and practical implementation. This writing plan outlines the detailed timeline, deliverables, quality assurance mechanisms, and resource allocation strategies necessary for successful completion of this multi-dimensional investigation into phonological features in the deep learning era.

This investigation addresses the fundamental question of what constitutes optimal representational units for language modeling in the deep learning era, considering multiple dimensions including predictive accuracy, linguistic interpretability, cognitive plausibility, and computational efficiency. The research bridges the gap between symbolic phonological theories and neural representations through systematic empirical studies and innovative neuro-symbolic architectures.

## Year 1: Foundation Phase (Months 1-12)

**Quarter 1 (Months 1-3)**: The initial quarter focuses on establishing theoretical foundations through comprehensive literature review covering three core areas: computational phonology [@tesar1995Computational; @jarosz2019Computational], self-supervised learning in speech [@baevski2020wav2vec; @mohamed2022SelfSupervised], and neuro-symbolic integration methodologies [@panchendrarajan2024Synergizing]. During this period, I will draft the theoretical framework chapter synthesizing insights from symbolic phonological theory and neural representation learning. Deliverables include an annotated bibliography of 100+ sources and a position paper outlining the research gap.

**Quarter 2 (Months 4-6)**: Implementation of baseline models and evaluation framework design constitutes the primary focus. This includes setting up Docker containers for reproducibility, implementing baseline SSL models (wav2vec 2.0, HuBERT, WavLM), and developing probing methodologies [@venkateswaran2025Probing]. The methodology chapter will be completed, incorporating detailed descriptions of experimental protocols and evaluation metrics. A workshop paper submission to a computational linguistics venue is planned.

**Quarter 3 (Months 7-9)**: Conducting Study 1 on representational landscape analysis comparing SSL models [@hsu2021HuBERT; @chen2022WavLM] with vector quantization approaches [@higy2021Discrete]. This involves systematic evaluation across phonological tasks using LibriSpeech and Common Voice datasets. The first empirical chapter draft will document findings. Results will be presented at the departmental seminar for feedback.

**Quarter 4 (Months 10-12)**: Beginning hybrid architecture development for Study 2, integrating Maximum Entropy Harmonic Grammar [@hayes2008Maximum] with neural networks. Technical implementation documentation will be maintained using version control. <!-- Conference submission preparation for ACL or INTERSPEECH is scheduled. -->

## Year 2: Core Development Phase (Months 13-24)

**Months 13-18**: Complete implementation and evaluation of hybrid neuro-symbolic architectures following principles from [@begus2020Generative; @chen2023Exploring]. Conduct comprehensive cross-linguistic evaluation using typologically diverse languages from PHOIBLE database. Draft second empirical chapter documenting architectural innovations and performance comparisons.

**Months 19-24**: Execute Study 3 on cognitive plausibility using CHILDES corpus [@cruzblandon2023Introducing] for developmental trajectory simulations. Implement ABX discrimination tasks and cross-linguistic transfer experiments. Complete third empirical chapter. <!-- Submit journal article to Computational Linguistics or TACL. -->

## Year 3: Consolidation Phase (Months 25-36)

**Months 25-30**: Conduct additional experiments addressing gaps identified through peer review. Write introduction chapter synthesizing theoretical foundations and general discussion chapter integrating findings across studies. Complete first full dissertation draft.

**Months 31-36**: Incorporate feedback from supervisor and committee members. Write conclusion chapter addressing broader impacts and future directions. Format dissertation according to university guidelines. Prepare and deliver defense presentation. Complete post-defense revisions.

## Quality Assurance and Milestones

Monthly progress meetings with primary supervisor ensure consistent advancement and timely problem resolution. Quarterly committee reviews provide comprehensive feedback on theoretical development and empirical progress. Regular presentations at lab meetings and reading groups facilitate peer feedback. All code and documentation maintained through Git version control ensures reproducibility. <!-- Target milestones include 2-3 conference papers (ACL, INTERSPEECH, NeurIPS) and 1 journal article during the thesis period. -->

## Resource Planning and Infrastructure

Computational resources include secured GPU cluster access through department allocation for large-scale experiments. Data licensing agreements established for Common Voice [@ardila2020Common] and other gated datasets. Software infrastructure includes Montreal Forced Aligner [@mcauliffe2017Montreal] for phonetic alignment, Kaldi toolkit for acoustic modeling, PyTorch for neural network implementation, and Hugging Face libraries for pretrained models. Planned research visits to affiliated laboratories provide specialized training and collaboration opportunities.

## Expected Outcomes and Deliverables

The completed dissertation will comprise a comprehensive investigation organized into theoretical foundations, methodological framework, empirical studies addressing the three research questions, and synthesis of findings with broader implications. Publications target high-impact venues in computational linguistics, speech processing, and machine learning. Open-source release of developed tools and models ensures broader research community impact. The research aims to establish new benchmarks for phonological representation learning and provide practical improvements for speech technology applications.

# References
